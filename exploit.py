from train import *


def exploit(exploiter_path, exploited_path, epoch, num_play, num_game, gamma=0.99, penalty=-1.0, batch_size=512):
    # model
    exploiter = tf.keras.models.load_model(exploiter_path) if exploiter_path.exists() else create_model()
    exploited = tf.keras.models.load_model(exploited_path)
    # dry run model in case model hasn't been built
    for model in [exploiter, exploited]:
        if not model.built:
            model.build(1)
    # optimizer
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
    # train_step
    input_signature = (
        tf.TensorSpec(shape=[None, None, 7], dtype=tf.int32), # vs
        tf.TensorSpec(shape=[None, None], dtype=tf.int32), # hs
        tf.TensorSpec(shape=[None], dtype=tf.int32), # ys
        tf.TensorSpec(shape=[None], dtype=tf.float32) # rs
    )
    grads_acc = [tf.Variable(tf.zeros_like(tv, dtype=tf.float32), trainable=False) for tv in exploiter.trainable_variables]
    metrices_acc = [tf.Variable(0.0, trainable=False) for _ in range(5)]
    @tf.function(input_signature=input_signature)
    def train_step(vs, hs, ys, rs):
        # reset metrices_acc
        for metric_acc in metrices_acc:
            metric_acc.assign(0.0)
        # split large tensor into batches
        dataset = tf.data.Dataset.from_tensor_slices((vs, hs, ys, rs)).batch(batch_size)
        # iterate over batches
        for vs_, hs_, ys_, rs_ in dataset:
            # compute metrices
            with tf.GradientTape() as tape:
                logits, values = exploiter(vs_, hs_, training=True)
                metrices = compute_loss(logits, values, ys_, rs_)
            loss = metrices[0]
            # compute grads
            grads = tape.gradient(loss, exploiter.trainable_variables)
            # accumulate grads
            for i in range(len(exploiter.trainable_variables)):
                if grads[i] is not None:
                    grads_acc[i].assign_add(grads[i])
            # accumulate metrices
            for metric, metric_acc in zip(metrices, metrices_acc):
                if metric is not None:
                    metric_acc.assign_add(metric)
            tf.print("metrices_acc:", metrices_acc)
        return tuple(metrices_acc)
    # training loop
    for e in range(epoch):
        # reset grads_acc
        for grad_acc in grads_acc:
            grad_acc.assign(tf.zeros_like(grad_acc))
        # run episodes
        losses = []
        losses_actor = []
        losses_critic = []
        probs = []
        expected_returns = []

        data_iterator = data_gen(num_play, num_game, model=exploiter, exploited=exploited)
        
        vs = defaultdict(list)
        hs = defaultdict(list)
        ys = defaultdict(list)
        rs = defaultdict(list)
        total = 15 * num_play * num_game # data only from player 0, the best & the worst player
        for episode in tqdm(data_iterator, total=total):
            vs_, hs_, ys_, rs_ = translate(episode, gamma=gamma, penalty=penalty)
            for key in vs_:
                vs[key].append(vs_[key])
                hs[key].append(hs_[key])
                ys[key].append(ys_[key])
                rs[key].append(rs_[key])
        # compute gradients & metrices
        for key in vs:
            v, h, y, r = [tf.concat(x, axis=0) for x in [vs[key], hs[key], ys[key], rs[key]]]
            tf.print("")
            tf.print("key:", key)
            tf.print("v.shape:", v.shape)
            tf.print("h.shape:", h.shape)
            tf.print("y.shape:", y.shape)
            tf.print("r.shape:", r.shape)
            loss, loss_actor, loss_critic, prob, expected_return  = train_step(v,h,y,r)
            losses.append(loss.numpy())
            losses_actor.append(loss_actor.numpy())
            losses_critic.append(loss_critic.numpy())
            probs.append(prob.numpy())
            expected_returns.append(expected_return.numpy())
        # compute the mean for all matrices in this epoch
        loss_avg = sum(losses)/total
        loss_actor_avg = sum(losses_actor)/total
        loss_critic_avg = sum(losses_critic)/total
        prob_avg = sum(probs)/total
        expected_return_avg = sum(expected_returns)/total
        print("epoch:", e)
        print("loss: %.2E" % loss_avg)
        print("loss actor: %.2E" % loss_actor_avg)
        print("loss critic: %.2E" % loss_critic_avg)
        print("prob: %.2E" % prob_avg)
        print("expected return: %.2E" % expected_return_avg)
        # apply grads for each epoch
        optimizer.apply_gradients(zip(grads_acc, exploiter.trainable_variables))
        # save exploiter
        exploiter.save(exploiter_path)
        print("exploiter saved")


if __name__ == "__main__":
    epoch = 1000
    num_play = 4 # The number of rehearsals for each game
    num_game = 4 # The number of games for each number of the total players
    # model
    model_dir = Path("model")
    if not model_dir.exists():
        model_dir.mkdir()
    exploiter_path = Path(model_dir, "exploiter.keras")
    exploited_path = Path(model_dir, "base.keras")
    exploit(exploiter_path, exploited_path, epoch, num_play, num_game)
