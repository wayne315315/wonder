import time
from train import *


def compute_loss_ppo(logits, values, actions, rewards, logits_old, epsilon=0.2, entropy_coeff=0.01):
    # inputs : v, h, y_true -> outputs : p(a|s), v(s)
    # inputs : rewards -> outputs: g(a,s)
    # vanilla loss_actor = -log(p(a|s)) * (g(a,s) - v(s))
    # ratio = p(a|s) / p_old(a|s)
    # loss_actor_ppo = -min(ratio * advantage_norm, clip(ratio, 1-epsilon, 1+epsilon) * advantage_norm)
    # loss_critic = huber_loss(g(a,s), v(s))
    # loss = loss_actor_ppo + loss_critic
    # TensorShape([None, 231]) TensorShape([None, 1]) TensorShape([None]) TensorShape([None]) TensorShape([None, 231])
    
    # loss critic
    huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)
    values = values[:,0] # v(s) TensorShape([None])
    loss_critic = huber_loss(rewards, values) # TensorShape([])
    advantages = rewards - tf.stop_gradient(values) # (g(a,s) - v(s))  TensorShape([None])
    # normalize advantages
    advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)

    # loss entropy
    logsoftmax = tf.nn.log_softmax(logits, axis=1) # log(p(ai|s), ...) TensorShape([None, 231])
    probs = tf.exp(logsoftmax) # p(ai|s) TensorShape([None, 231])
    loss_entropy = tf.reduce_sum(probs * logsoftmax) # we wanna maximize entropy

    # loss actor ppo
    logsoftmax = tf.gather(logsoftmax, actions, batch_dims=1) # log(p(a|s)) TensorShape([None])
    logsoftmax_old = tf.nn.log_softmax(logits_old, axis=1) # log(p_old(ai|s), ...) TensorShape([None, 231])
    logsoftmax_old = tf.gather(logsoftmax_old, actions, batch_dims=1) # log(p_old(a|s)) TensorShape([None])
    ratio = tf.exp(logsoftmax - logsoftmax_old)
    surr1 = ratio * advantages
    surr2 = tf.clip_by_value(ratio, 1.0 - epsilon, 1.0 + epsilon) * advantages
    loss_actor_ppo = -tf.reduce_sum(tf.minimum(surr1, surr2))

    # overall loss
    loss = loss_actor_ppo + loss_critic + entropy_coeff * loss_entropy

    # metrics
    probs = tf.gather(probs, actions, batch_dims=1) # p(a|s) TensorShape([None])
    expected_return = tf.reduce_sum(probs * rewards) # E[R|a,s] TensorShape([])
    return loss, loss_actor_ppo, loss_critic, loss_entropy, expected_return


def exploit(exploiter_path, exploited_path, epoch, num_play, num_game, steps=2, learning_rate=1e-4, gamma=0.99, penalty=-1.0, batch_size=512):
    # model
    exploiter = tf.keras.models.load_model(exploiter_path) if exploiter_path.exists() else create_model()
    exploited = tf.keras.models.load_model(exploited_path)
    # dry run model in case model hasn't been built
    for model in [exploiter, exploited]:
        if not model.built:
            model.build(1)

    # export base model with serve_name
    serve_exploiter = "exploiter"
    serve_exploited = "exploited"
    export_archive(serve_exploiter, exploiter, 0)
    export_archive(serve_exploited, exploited, 0)
    # launch server
    launch_server()

    # optimizer
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    # train_step
    input_signature = (
        tf.TensorSpec(shape=[None, None, 7], dtype=tf.int32), # vs
        tf.TensorSpec(shape=[None, None], dtype=tf.int32), # hs
        tf.TensorSpec(shape=[None], dtype=tf.int32), # ys
        tf.TensorSpec(shape=[None], dtype=tf.float32), # rs
        tf.TensorSpec(shape=[None, 3 * exploiter.num_card], dtype=tf.float32) # ls
    )
    grads_acc = [tf.Variable(tf.zeros_like(tv, dtype=tf.float32), trainable=False) for tv in exploiter.trainable_variables]
    metrices_acc = [tf.Variable(0.0, trainable=False) for _ in range(5)]
    @tf.function(input_signature=input_signature)
    def train_step(vs, hs, ys, rs, ls):
        # reset metrices_acc
        for metric_acc in metrices_acc:
            metric_acc.assign(0.0)
        # split large tensor into batches
        dataset = tf.data.Dataset.from_tensor_slices((vs, hs, ys, rs, ls)).batch(batch_size)
        # iterate over batches
        for vs_, hs_, ys_, rs_, ls_ in dataset:
            # compute metrices
            with tf.GradientTape() as tape:
                logits, values = exploiter(vs_, hs_, training=True)
                metrices = compute_loss_ppo(logits, values, ys_, rs_, ls_)
            loss = metrices[0]
            # compute grads
            grads = tape.gradient(loss, exploiter.trainable_variables)
            # accumulate grads
            for i in range(len(exploiter.trainable_variables)):
                if grads[i] is not None:
                    grads_acc[i].assign_add(grads[i])
            # accumulate metrices
            for metric, metric_acc in zip(metrices, metrices_acc):
                if metric is not None:
                    metric_acc.assign_add(metric)
        return tuple(metrices_acc)
    # training loop
    exploiter_best_path = Path(exploiter_path.parent, "exploiter_best.keras")
    expected_return_avg_best = float('-inf')
    for e in range(epoch):
        # data collection from real game play
        print(f"=== Wait for serving version {e} ===")
        t1 = time.time()
        while not probe(serve_exploiter, serve_version=e) or not probe(serve_exploited):
            time.sleep(1)
        t2 = time.time()
        print(f"=== Serving version {e} launched ===")
        print("Serving version took %.2f seconds" % (t2-t1))
        data_iterator = data_gen(num_play, num_game, serve_name=serve_exploiter, serve_version=e, exploited=serve_exploited)
        vs = defaultdict(list)
        hs = defaultdict(list)
        ys = defaultdict(list)
        rs = defaultdict(list)
        ls = defaultdict(list) # old logits
        total = 5 * num_play * num_game # data only from player 0
        print("=== Start generating epsiodes ===")
        t1 = time.time()
        for episode in tqdm(data_iterator, total=total):
            vs_, hs_, ys_, rs_ = translate(episode, gamma=gamma, penalty=penalty)
            for key in vs_:
                vs[key].append(vs_[key])
                hs[key].append(hs_[key])
                ys[key].append(ys_[key])
                rs[key].append(rs_[key])
        t2 = time.time()
        print("=== End generating episodes ===")
        print("Generating Episodes took %.2f seconds" % (t2-t1))
        # concatenate all tensors for each key
        print("=== Start computing prior logits ===")
        t1 = time.time()
        for key in vs:
            for item in [vs, hs, ys, rs]:
                item[key] = tf.concat(item[key], axis=0)
            #logits, _ = exploiter.predict([vs[key], hs[key]], batch_size=batch_size)
            logits, _ = exploiter(vs[key], hs[key], training=False)
            ls[key] = logits
        t2 = time.time()
        print("=== End computing prior logits ===")
        print("Computing prior logits took %.2f seconds" % (t2-t1))
        ### Start training ###
        print("=== Start training ===")
        t1 = time.time()
        for step in range(steps):
            # reset grads_acc
            for grad_acc in grads_acc:
                grad_acc.assign(tf.zeros_like(grad_acc))
            # replay
            losses = []
            losses_actor = []
            losses_critic = []
            losses_entropy = []
            expected_returns = []
            # compute gradients & metrices
            for key in vs:
                v, h, y, r, l = [vs[key], hs[key], ys[key], rs[key], ls[key]]
                tf.print("")
                tf.print("key:", key)
                loss, loss_actor, loss_critic, loss_entropy, expected_return  = train_step(v,h,y,r,l)
                losses.append(loss.numpy())
                losses_actor.append(loss_actor.numpy())
                losses_critic.append(loss_critic.numpy())
                losses_entropy.append(loss_entropy.numpy())
                expected_returns.append(expected_return.numpy())
                n = v.shape[0]
                tf.print("batch size:", n)
                tf.print("loss:", loss/n)
                tf.print("loss actor:", loss_actor/n)
                tf.print("loss critic:", loss_critic/n)
                tf.print("loss entropy:", loss_entropy/n)
                tf.print("expected return:", expected_return/n)

            # compute the mean for all matrices in this epoch
            loss_avg = sum(losses)/total
            loss_actor_avg = sum(losses_actor)/total
            loss_critic_avg = sum(losses_critic)/total
            loss_entropy_avg = sum(losses_entropy)/total
            expected_return_avg = sum(expected_returns)/total
            print("epoch:", e)
            print("step:", step)
            print("loss: %.2E" % loss_avg)
            print("loss actor: %.2E" % loss_actor_avg)
            print("loss critic: %.2E" % loss_critic_avg)
            print("loss entropy: %.2E" % loss_entropy_avg)
            print("expected return: %.2E" % expected_return_avg)
            # save exploiter
            exploiter.save(exploiter_path)
            print("exploiter saved")
            # save the best exploiter only on the first step if the condition met
            if (step == 0) and expected_return_avg > expected_return_avg_best:
                expected_return_avg_best = expected_return_avg
                exploiter.save(exploiter_best_path)
                print("exploiter best saved")
            # apply grads for each epoch
            optimizer.apply_gradients(zip(grads_acc, exploiter.trainable_variables))
        t2 = time.time()
        print("=== End training ===")
        print("End training took %.2f seconds" % (t2-t1))
        ### End training ###

        # export the updated exploiter with version e
        export_archive(serve_exploiter, exploiter, e+1)

if __name__ == "__main__":
    epoch = 1000
    num_play = 2 # The number of rehearsals for each game
    num_game = 2 # The number of games for each number of the total players
    # model
    model_dir = Path("model")
    if not model_dir.exists():
        model_dir.mkdir()
    exploiter_path = Path(model_dir, "exploiter.keras")
    exploited_path = Path(model_dir, "base.keras")

    try:
        exploit(exploiter_path, exploited_path, epoch, num_play, num_game, learning_rate=1e-5)
    except Exception as e:
        raise e
    finally:
        # clean archive
        for serve_name in ["exploiter", "exploited"]:
            clean_archive(serve_name)
        # kill server
        kill_server()
